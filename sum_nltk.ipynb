{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\puran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\puran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading word: Package 'word' not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t', 'were', 'its', 'needn', 'i', 'if', 'during', \"aren't\", 'most', 'but', 'few', 'above', 'won', \"wouldn't\", \"it's\", 're', 've', 'y', 'through', 'now', 'no', 'weren', 'she', 'was', 'once', 'can', 'an', 'is', 'didn', 'each', 'the', \"you'd\", 'ain', \"weren't\", 'whom', 'very', \"you'll\", 'wouldn', 'by', 'down', \"mustn't\", \"wasn't\", 'then', 'wasn', 'been', \"that'll\", 'hers', 'being', 'such', 'about', 'are', 'it', 'other', 'between', 'to', \"don't\", 'which', 'in', 'out', 'own', 'have', \"won't\", 'all', 'd', 'before', 'they', 'for', 'shan', \"she's\", 'has', 'his', 'me', 'only', \"you're\", 'herself', 'mustn', 'so', 'itself', 'further', 'll', 'yourselves', 'shouldn', 'having', 'some', 'should', 'my', 'what', 'this', 'on', 'those', 'both', 'off', 'any', 'a', 'ourselves', \"didn't\", 'be', \"doesn't\", 'while', \"shouldn't\", \"should've\", 'their', 'yourself', 'after', 'doing', \"needn't\", 'or', 'against', 'o', 'ma', 'same', 'into', 'did', 'him', \"haven't\", 'where', 'themselves', 'how', 'does', 'here', 'our', 'because', 'under', 'hasn', 'had', 'and', 'do', 'that', \"hadn't\", 'with', 'he', 'couldn', 'theirs', 'don', 'we', 'your', 'too', 'just', 'more', 'hadn', \"couldn't\", 'mightn', 'not', 'myself', 'isn', 'yours', 'there', 'ours', \"mightn't\", 'haven', 'm', 'when', 'up', 'over', 'from', \"isn't\", 'doesn', 'of', 'am', 'aren', 'until', 'at', 'himself', 'below', \"you've\", 's', 'nor', 'you', 'them', 'why', 'will', 'again', 'who', 'as', 'than', \"shan't\", \"hasn't\", 'these', 'her'}\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "print(stop)\n",
    "punct = punctuation + '\\n'\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    encoded_text = text.encode(\"utf-8\")\n",
    "    text = encoded_text.decode(\"utf-8\")\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency(processed_text):\n",
    "    word_frequency_dictionary = {}\n",
    "\n",
    "    for word in processed_text:\n",
    "        word = word.lower()\n",
    "        \n",
    "        if word not in stop:\n",
    "            if word not in punct:\n",
    "                if word not in word_frequency_dictionary:\n",
    "                    word_frequency_dictionary[word] = 1\n",
    "                else:\n",
    "                    word_frequency_dictionary[word] += 1\n",
    "\n",
    "    return word_frequency_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_frequency(word_frequency_dictionary):\n",
    "    max_freq = max(word_frequency_dictionary.values())\n",
    "    \n",
    "    return max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizing(word_frequency_dictionary, max_freq):\n",
    "    for word in word_frequency_dictionary:\n",
    "        word_frequency_dictionary[word] = word_frequency_dictionary[word]/max_freq\n",
    "        \n",
    "    return word_frequency_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_score(word_frequency_dictionary, text):\n",
    "    sentence_score_dictionary = {}\n",
    "\n",
    "    encoded_text = text.encode(\"utf-8\")\n",
    "    text = encoded_text.decode(\"utf-8\")\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "\n",
    "            if word in word_frequency_dictionary:\n",
    "                if sentence not in sentence_score_dictionary:\n",
    "                    sentence_score_dictionary[sentence] = word_frequency_dictionary[word]\n",
    "                else:\n",
    "                    sentence_score_dictionary[sentence] += word_frequency_dictionary[word]\n",
    "                    \n",
    "    return sentence_score_dictionary\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(text):\n",
    "    length = int(len(sent_tokenize(text))*0.3)\n",
    "    \n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(sl, sts):\n",
    "    summary = nlargest(sl, sts)\n",
    "    summary = ' '.join(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" It's a 5v5 multiplayer first-person shooter (FPS) where one team attacks and the other defends. The main game mode, Search and Destroy, is very similar to CS:GO. The attacking team's goal is to plant a bomb (called a spike) and have it detonate, while the defending team tries to avoid that. Regardless of whether the spike is planted or not, if a squad is wiped out before any other victory condition is met, the opposing squad will win.\n",
    "Matches are 25 rounds long, with each round lasting 100 seconds. The first team to win 13 rounds wins the match overall. At the beginning of the round, you'll have 30 seconds to buy weapons and gear for that round. If you die in a round, you'll have to wait until the next round to respawn. This main game mode can be played in either unrated or ranked matches.\n",
    "It sounds just like CS:GO's Defuse game mode, but Riot has a twist on the formula. In addition to buying weapons, you'll also choose an Agent at the beginning of each round. Each Agent has an ability, ranging from healing allies to making walls appear out of nowhere. Considering Valorant takes a MOBA-based approach to Agents, you'll likely need a good spread of abilities if you want to win a match. Note, however, that you'll need to buy some abilities at the beginning of a round.\n",
    "Since launch, Valorant has added the Spike Rush game mode. This is a quicker, best-of-seven-rounds mode where every attacker is equipped with a spike, and all players have the same weapon and all of their abilities; there is no buy round.\n",
    "Valorant's seasons will be known as Acts, each bringing new additions to the game including new Agents, maps, and modes \"\"\"\n",
    "\n",
    "pt = preprocessing(text)\n",
    "wfd = word_frequency(pt)\n",
    "mf = max_frequency(wfd)\n",
    "vtr = vectorizing(wfd, mf)\n",
    "sts = sentence_score(wfd, text)\n",
    "sl = sentence_length(text)\n",
    "summary = get_summary(sl, sts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
